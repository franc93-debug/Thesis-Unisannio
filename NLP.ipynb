{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzPAcuPK428q"
      },
      "outputs": [],
      "source": [
        "from nrclex import NRCLex\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.sparse import hstack\n",
        "import gensim.corpora as corpora\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import gensim\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"prdataset.csv\")\n",
        "df_train = pd.read_csv('train_.csv')\n",
        "df_test = pd.read_csv('test_.csv')"
      ],
      "metadata": {
        "id": "nHa2KwmI462M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "tokens = []\n",
        "for i in df_train['processed_text']:\n",
        "    word_tokens = word_tokenize(i)\n",
        "    tokens.append(word_tokens)\n",
        "\n",
        "tokens_all = []\n",
        "for i in df['processed_text']:\n",
        "    word_tokens = word_tokenize(i)\n",
        "    tokens_all.append(word_tokens)"
      ],
      "metadata": {
        "id": "Yt4_PYbv4-W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# emozioni primarie e sentiment\n",
        "emotion_list = []\n",
        "for i in df['processed_text']:\n",
        "    emotion = NRCLex(i)\n",
        "    emotion_list.append(emotion.affect_frequencies)\n",
        "\n",
        "\n",
        "em_dict = pd.DataFrame(emotion_list)\n",
        "em_dict1 = em_dict.drop(columns='anticip')\n",
        "\n",
        "new_list = pd.DataFrame()\n",
        "for i in em_dict1:\n",
        "    new_list = pd.concat([new_list,em_dict1[i].fillna(0)], axis=1)\n",
        "\n",
        "new_list = pd.concat([new_list, df['label']], axis=1)\n",
        "new_list.to_csv('emotions_data.csv', index=False)\n",
        "\n",
        "\n",
        "### LATENT DIRICHLET ALLOCATION\n",
        "lem = WordNetLemmatizer()\n",
        "train_data_lda = []\n",
        "for i in tokens:\n",
        "    new_lem = []\n",
        "    for words in i:\n",
        "        new_lem.append(lem.lemmatize(words))\n",
        "    train_data_lda.append(new_lem)\n",
        "\n",
        "data_lda = []\n",
        "for i in tokens_all:\n",
        "    new_lem = []\n",
        "    for words in i:\n",
        "        new_lem.append(lem.lemmatize(words))\n",
        "    data_lda.append(new_lem)\n",
        "\n",
        "\n",
        "id2word = corpora.Dictionary(train_data_lda)\n",
        "id2word_all= corpora.Dictionary(data_lda)\n",
        "id2word_all.filter_extremes(no_below=5)\n",
        "\n",
        "corpus = [id2word_all.doc2bow(text) for text in train_data_lda]\n",
        "corpus_all = [id2word_all.doc2bow(text) for text in data_lda]\n",
        "\n",
        "random.seed(12345)\n",
        "lda_train = gensim.models.LdaModel(\n",
        "                            corpus=corpus,\n",
        "                            num_topics=8,\n",
        "                            id2word=id2word_all, eta=0.1, iterations=100)\n",
        "lda_train.show_topics()\n",
        "\n",
        "t = lda_train.update(corpus_all)\n",
        "\n",
        "top_names = [0,1,2,3,4,5,6,7]\n",
        "topics = pd.DataFrame(index=df.index,columns=top_names)\n",
        "for i in range(len(corpus_all)):\n",
        "    tops = lda_train.get_document_topics(corpus_all[i])\n",
        "    for j in tops:\n",
        "        topics.iloc[i,j[0]]=j[1]\n",
        "\n",
        "topics = topics.fillna(0)\n",
        "topics.columns = ['PoliticaEstera','Attualita','CampagnaTrump','Russia',\n",
        "                  'Repubblicani','Arresti','InvestigazioneClinton','Societa']\n",
        "topics.to_csv('topics.csv',index=False)\n",
        "\n",
        "tops_train = lda_train.get_document_topics(corpus)\n",
        "\n",
        "\n",
        "\n",
        "##### CREAZIONI VARIABILI ESPLICATIVE\n",
        "\n",
        "#1 lunghezza del testo\n",
        "length = []\n",
        "for i in df['text']:\n",
        "    length.append(len(i))\n",
        "\n",
        "\n",
        "#2,3 freq parole di lunghezza >11, <7\n",
        "# testo processato ma senza stemming\n",
        "freq11 = []\n",
        "freq5 = []\n",
        "# freq14 = []\n",
        "# freq15 = []\n",
        "for i in tokens:\n",
        "    c11 = 0\n",
        "    c7 = 0\n",
        "    # c14 = 0\n",
        "    # c15 = 0\n",
        "    for t in i:\n",
        "        if len(t) >= 11:\n",
        "            c11 += 1\n",
        "        elif len(t) <= 5:\n",
        "            c7 += 1\n",
        "        # elif len(t) == 14:\n",
        "        #     c14 += 1\n",
        "        # elif len(t) >= 15:\n",
        "        #     c15 += 1\n",
        "    freq11.append(c11/len(i))\n",
        "    freq5.append(c7/len(i))\n",
        "    freq14.append(c14/len(i))\n",
        "    freq15.append(c15/len(i))\n",
        "\n",
        "#4,5,6 numero medio di parole per frase, % punteggiatura, numero citazioni\n",
        "# testo non processato\n",
        "tokens_np = []\n",
        "for i in df['text']:\n",
        "    word_tokens = word_tokenize(i)\n",
        "    tokens_np.append(word_tokens)\n",
        "\n",
        "\n",
        "nwords_tot = []\n",
        "npunct_tot = []\n",
        "nquotes_tot = []\n",
        "for i in tokens_np:\n",
        "    count = 0\n",
        "    npunct = 0\n",
        "    nquotes = 0\n",
        "    nwords = []\n",
        "    for x in i:\n",
        "        if x =='.' or x ==';' or x =='!' or x=='...' or x==':':\n",
        "            nwords.append(count)\n",
        "            count = 0\n",
        "            npunct += 1\n",
        "        elif x == '“':\n",
        "            npunct += 1\n",
        "            nquotes += 1\n",
        "        elif x ==',':\n",
        "            npunct += 1\n",
        "        else:\n",
        "            count = count + 1\n",
        "    if nwords == []:\n",
        "        nwords.append(count)\n",
        "    npunct_tot.append(npunct)\n",
        "    nquotes_tot.append(nquotes)\n",
        "    nwords_tot.append(nwords)\n",
        "\n",
        "\n",
        "def Average(lst):\n",
        "    return sum(lst)/len(lst)\n",
        "\n",
        "av_nwords = []\n",
        "for i in nwords_tot:\n",
        "    av_nwords.append(Average(i))\n",
        "\n",
        "# numero di lettere maiuscole\n",
        "nupper = []\n",
        "for i in tokens:\n",
        "    s = 0\n",
        "    for x in i:\n",
        "        s += sum(1 for c in x if c.isupper())\n",
        "    nupper.append(s)\n",
        "\n",
        "\n",
        "#7 BAG OF WORDS, TFIDF\n",
        "\n",
        "# calcolo top words nelle fake news\n",
        "df_fake=df_train[df_train['label']==1]\n",
        "fake_vect = TfidfVectorizer()\n",
        "count = fake_vect.fit_transform(df_fake['processed_text'])\n",
        "fake_words = list(fake_vect.get_feature_names())\n",
        "dtm_fake = pd.DataFrame(count.todense(), columns = fake_words)\n",
        "trans_f = count.transpose()\n",
        "sm_f=[]\n",
        "sm_f = trans_f.sum(axis=1)\n",
        "t = pd.DataFrame()\n",
        "names = pd.DataFrame(fake_words)\n",
        "sums = pd.DataFrame(sm_f)\n",
        "t = pd.concat([names,sums], axis=1)\n",
        "t.columns = ['names','sums']\n",
        "\n",
        "sm = t.getcol(0).todense()\n",
        "top = np.sort(sm,axis=0)[::-1]\n",
        "\n",
        "top_fake_m = t.sort_values(by='sums', ascending=False).head(20)\n",
        "top_fake = list(top_fake_m['names'])\n",
        "\n",
        "dtm corpus\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf = vectorizer.fit_transform(df['processed_text'])\n",
        "top_words = list(vectorizer.get_feature_names())\n",
        "dtm = pd.DataFrame(tfidf.todense(), columns=top_fake)\n",
        "tfidf.gelcol(top_fake[0])\n",
        "j = 0\n",
        "ind=[]\n",
        "for x in top_fake:\n",
        "    for i in range(len(top_words)):\n",
        "        if top_words[i] == x:\n",
        "            ind.append(i)\n",
        "\n",
        "\n",
        "d = pd.DataFrame()\n",
        "for i in ind:\n",
        "    c = tfidf[:,i]\n",
        "    c = pd.DataFrame(c.todense())\n",
        "    d = pd.concat([d,c], axis = 1)\n",
        "\n",
        "d.columns = top_fake\n",
        "d.to_csv('tfidf.csv',index=False)\n",
        "\n",
        "\n",
        "# TOPIC MODELLING\n",
        "\n",
        "# CREAZIONE NUOVO DATA FRAME CON VARIABILI ESPLICATIVE\n",
        "df1 = pd.DataFrame(\n",
        "    {'label' : list(df['label']),\n",
        "     'length' : length,\n",
        "     'perc5' : freq5,\n",
        "     'perc11' : freq11,\n",
        "     'av_words' : av_nwords,\n",
        "     'punct': npunct_tot,\n",
        "     'quotes':nquotes_tot\n",
        "     })\n",
        "\n",
        "d = pd.read_csv('tfidf.csv')\n",
        "df2 = pd.concat([df1,d], axis=1)\n",
        "df3 = pd.concat([df2,d_em], axis=1)\n",
        "df2 = pd.concat([df1a, new_list], axis = 1)\n",
        "# csv che servirà per l'applicazione dei modelli\n",
        "df3.to_csv('modelling_data_wel.csv', index = False)\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_csv('modelling_data1.csv')\n",
        "data[]\n",
        "emotions = pd.read_csv('emotions_data.csv')\n",
        "d_em = emotions.drop(columns='label')\n",
        "topics = pd.read_csv('topics.csv')\n",
        "data1 = pd.concat([data,d], axis=1)\n",
        "data2 = pd.concat([data1, topics], axis = 1)\n",
        "data2.to_csv('modelling_data1_wel.csv', index = False)\n",
        "##\n",
        "data = pd.read_csv('modelling_data1_wel.csv')\n",
        "data1 = pd.concat([df['Unnamed: 0'], data], axis=1)\n",
        "data1.columns = ['index', 'label', 'length', 'perc5', 'perc11', 'av_words', 'punct',\n",
        "       'quotes', 'trump', 'us', 'people', 'said', 'one', 'would', 'like',\n",
        "       'also', 'president', 'even', 'time', 'clinton', 'state', 'new', 'many',\n",
        "       'could', 'donaldtrump', 'government', 'media', 'get', 'fear', 'anger',\n",
        "       'trust', 'surprise', 'positive', 'negative', 'sadness', 'disgust',\n",
        "       'joy', 'anticipation']\n",
        "\n",
        "data1.to_csv('modelling_data1.csv', index = False)\n",
        "\n",
        "# train e test per modelling data\n",
        "\n",
        "train = pd.DataFrame()\n",
        "for i in df_train['Unnamed..0']:\n",
        "    train = pd.concat([train,data.loc[data['index']==i]], axis=0)\n",
        "\n",
        "test = pd.DataFrame()\n",
        "for i in df_test['Unnamed..0']:\n",
        "    test = pd.concat([test,data.loc[data['index']==i]], axis=0)\n",
        "\n",
        "test.to_csv('test_modelling.csv', index=False)\n",
        "train.to_csv('train_modelling.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "cBSF2olG5Cuk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}